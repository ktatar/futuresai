@inproceedings{cotton_singing_2024,
	language = {en},
	author = {Cotton, Kelsey and de Vries, Katja and Tatar, Kƒ±van√ß},
	month = May,
	bibtex_show={true},
	selected={true},
	title = {Singing for the Missing: Bringing the Body Back to AI Voice and Speech Technologies},
	year = {2024},
	isbn = {9798400709944},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3658852.3659065},
	doi = {10.1145/3658852.3659065},
	abstract = {Technological advancements in deep learning for speech and voice have contributed to a recent expansion in applications for voice cloning, synthesis and generation. Invisibilised stakeholders in this expansion are numerous absent bodies, whose voices and voice data have been integral to the development and refinement of these speech technologies. This position paper probes current working practices for voice and speech in machine learning and AI, in which the bodies of voices are ‚Äúinvisibilised". We examine the facts and concerns about the voice-Body in applications of AI-voice technology. We do this through probing the wider connections between voice data and Schaefferian listening; speculating on the consequences of missing Bodies in AI-Voice; and by examining how vocalists and artists working with synthetic Bodies and AI-voices are ‚Äòbringing the Body back‚Äô in their own practices. We contribute with a series of considerations for how practitioners and researchers may help to ‚Äòbring the Body back‚Äô into AI-voice technologies.},
	booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
	articleno = {2},
	numpages = {12},
	keywords = {AI, STS, artificial intelligence, body, musical AI, voice},
	location = {Utrecht, Netherlands},
	series = {MOCO '24}
}

@inproceedings{cotton_sounding_2024,
	title = {Sounding out extra-normal AI voice: Non-normative musical engagements with normative AI voice and speech technologies},
	abstract = {How do we challenge the norms of AI voice technologies? What would be a non-normative approach in finding novel artistic possibilities of speech synthesis and text-to-speech with Deep Learning? This paper delves into SpeechBrain, OpenAI and CoquiTTS voice and speech models with the perspective of an experimental vocal practitioner. An exploratory Research-through-Design process guided an engagement with pre-trained speech synthesis models to reveal their musical affordances in an experimental vocal practice. We recorded this engagement with voice and speech Deep Learning technologies using auto-ethnography, a novel and recent methodology in Human-Computer Interaction. Our position in this paper actively subverts the normative function of these models, provoking nonsensical AI-mediation of human vocality. Emerging from a sense-making process of poetic AI nonsense, we uncover the generative potential of non-normative usage of normative speech recognition and synthesis models. We contribute with insights about the affordances of Research-through-Design to inform artistic processes in working with AI models; how AI-mediations reform understandings of human vocality; and artistic perspectives and practice as knowledge-creation mechanisms for working with technology.},
	language = {en},
	booktitle = {AIMC 2024},
	author = {Cotton, Kelsey and Tatar, Kƒ±van√ß},
	month = aug,
	year = {2024},
	url = {https://aimc2024.pubpub.org/pub/extranormal-aivoice/release/1}
}
@inproceedings{erenCoquiTTS2021,
  title = {Coqui {{TTS}}},
  author = {Eren, G√∂lge and {The Coqui TTS Team}},
  date = {2021-01},
  doi = {10.5281/zenodo.6334862},
  url = {https://github.com/coqui-ai/TTS},
  urldate = {2024-08-08},
  abstract = {üê∏üí¨ - a deep learning toolkit for Text-to-Speech, battle-tested in research and production},
  version = {1.4}
}
@inproceedings{speechbrain,
  title={{SpeechBrain}: A General-Purpose Speech Toolkit},
  author={Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and Fran√ßois Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio},
  year={2021},
  eprint={2106.04624},
  archivePrefix={arXiv},
  primaryClass={eess.AS},
  note={arXiv:2106.04624}
}
@inproceedings{radford,
  title = {Robust {{Speech Recognition}} via {{Large-Scale Weak Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  year = {2022},
  eprint = {2212.04356},
  archivePrefix={arXiv},
  primaryClass={eess.AS},
  url = {http://arxiv.org/abs/2212.04356},
}

